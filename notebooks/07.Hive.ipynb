{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Мотивация \n",
    "- Написание Hadoop-задач на Java нетривиально\n",
    "- Хочется использовать возможность Hadoop для обработки слишком больших для обычной реляционной БД данных\n",
    "- Хочется иметь определенную схему данных\n",
    "- SQL? Да - HiveSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Архитектура"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Hive](https://cwiki.apache.org/confluence/download/attachments/27362072/system_architecture.png)\n",
    "\n",
    "- `Metastore` хранит метаинформацию о таблицах, колонках и их типах, где и как они хранятся и т.д.\n",
    "    - В качестве хранилища может использоваться реляционная БД\n",
    "- `Driver`- компонент, который обслуживает жизненный цикл запроса к `Hive`.\n",
    "- `Query Compiler` - компонент, который обрабатывает запроса на `HiveSQL` и преобразует его в последовательность MapReduce-задач\n",
    "- `Execution Engine` - компонент, который непосредственно запускает MapReduce-задачи. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Клиент Python\n",
    "\n",
    "\n",
    "```bash\n",
    "python3 -m pip install pyhive[hive_pure_sasl]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('John2', 1), ('Mike2', 1)]\n"
     ]
    }
   ],
   "source": [
    "from pyhive import hive  \n",
    "with hive.connect('localhost', username='root') as conn:\n",
    "    with conn.cursor() as cursor:        \n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS person (               \n",
    "                n STRING, \n",
    "                age INT\n",
    "            )\n",
    "            ''')\n",
    "        \n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "            INSERT INTO TABLE person VALUES (\"John2\", 550), (\"Mike2\", 800)\n",
    "        ''')\n",
    "     \n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''\n",
    "            SELECT n, count(*) FROM person group by n\n",
    "        ''')\n",
    "        \n",
    "        print(cursor.fetchall())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Хранение данных\n",
    "\n",
    "- База Данных - это набор Hive-таблиц\n",
    "    - физически представляет собой директорию на HDFS\n",
    "- Таблицы в базе данных \n",
    "    - таблицы физически хранятся в поддиректориях\n",
    "    - метаинформация о таблице хранится в `Metastore`\n",
    "\n",
    "Создать базу дынных:\n",
    "```sql \n",
    "CREATE DATABASE test_db;\n",
    "```\n",
    "   \n",
    "Создать таблицу:\n",
    "```sql\n",
    "CREATE TABLE weather ( \n",
    "               dt TIMESTAMP, \n",
    "               t FLOAT, \n",
    "               po FLOAT,\n",
    "               p FLOAT, \n",
    "               u FLOAT, \n",
    "               vv FLOAT, \n",
    "               td float, \n",
    "               n STRING) \n",
    "           ROW FORMAT DELIMITED FIELDS TERMINATED BY ','; \n",
    "               \n",
    "LOAD DATA LOCAL INPATH '/course/data/weather_stat.csv' INTO TABLE weather;               \n",
    "```\n",
    "\n",
    "\n",
    "```sql\n",
    "CREATE TABLE cik ( \n",
    "               region STRING, \n",
    "               tik STRING, \n",
    "               uik STRING, \n",
    "               voters_total INT,\n",
    "               total INT, \n",
    "               total_ahead INT,\n",
    "               total_inside INT, \n",
    "               total_outside INT, \n",
    "               total_removed INT,\n",
    "               outside INT,\n",
    "               inside INT, \n",
    "               invalid INT,\n",
    "               valid INT, \n",
    "               lost INT,\n",
    "               unkwn INT,\n",
    "    \n",
    "               baburin INT,\n",
    "               grudinin INT, \n",
    "               zhirinovsky INT, \n",
    "               putin INT, \n",
    "               sobchak INT,\n",
    "               suraykin INT,\n",
    "               titov INT, \n",
    "               yavlinsky INT\n",
    ") ROW FORMAT DELIMITED FIELDS TERMINATED BY ','; \n",
    "               \n",
    "LOAD DATA LOCAL INPATH '/course/data/cik_trunc.csv' INTO TABLE cik;               \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Типы данных\n",
    "\n",
    "##### Numeric Types\n",
    "\n",
    "    TINYINT (1-byte signed integer)\n",
    "    SMALLINT (2-byte signed integer)\n",
    "    INT/INTEGER (4-byte signed integer)\n",
    "    BIGINT (8-byte signed integer)\n",
    "    FLOAT (4-byte single precision floating point number)\n",
    "    DOUBLE (8-byte double precision floating point number)\n",
    "    DECIMAL\n",
    "    NUMERIC \n",
    "\n",
    "##### Date/Time Types\n",
    "\n",
    "    TIMESTAMP \n",
    "    DATE \n",
    "    INTERVAL\n",
    "\n",
    "##### String Types\n",
    "\n",
    "    STRING\n",
    "    VARCHAR \n",
    "    CHAR \n",
    "\n",
    "##### Misc Types\n",
    "\n",
    "    BOOLEAN\n",
    "    BINARY \n",
    "\n",
    "##### Complex Types\n",
    "    \n",
    "    arrays: ARRAY<data_type> \n",
    "    maps: MAP<primitive_type, data_type> \n",
    "    structs: STRUCT<col_name : data_type [COMMENT col_comment], ...>\n",
    "    union: UNIONTYPE<data_type, data_type, ...> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание таблиц\n",
    "\n",
    "Колонки можно разделять с помощью регулярных выражений\n",
    "\n",
    "```sql\n",
    "CREATE TABLE apachelog (\n",
    "      host STRING,\n",
    "      identity STRING,\n",
    "      user STRING,\n",
    "      time STRING,\n",
    "      request STRING,\n",
    "      status STRING,\n",
    "      size STRING,\n",
    "      referer STRING,\n",
    "      agent STRING)\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'\n",
    "WITH SERDEPROPERTIES (\n",
    "  \"input.regex\" = \"([^]*) ([^]*) ([^]*) (-|\\\\[^\\\\]*\\\\]) ([^ \\\"]*|\\\"[^\\\"]*\\\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \\\"]*|\\\".*\\\") ([^ \\\"]*|\\\".*\\\"))?\"\n",
    ")\n",
    "STORED AS TEXTFILE;\n",
    "```\n",
    "\n",
    "Для кастомного CSV\n",
    "\n",
    "```sql\n",
    "CREATE TABLE my_table(a string, b string, ...)\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "WITH SERDEPROPERTIES (\n",
    "   \"separatorChar\" = \"\\t\",\n",
    "   \"quoteChar\"     = \"'\",\n",
    "   \"escapeChar\"    = \"\\\\\"\n",
    ")  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning\n",
    "\n",
    "Физически партиции хранятся в разных директориях\n",
    "\n",
    "```sql \n",
    "\n",
    "CREATE TABLE page_view(viewTime INT, userid BIGINT,\n",
    "     page_url STRING, referrer_url STRING,\n",
    "     ip STRING COMMENT 'IP Address of the User')\n",
    " COMMENT 'This is the page view table'\n",
    " PARTITIONED BY(dt STRING, country STRING)\n",
    " STORED AS SEQUENCEFILE;\n",
    "```\n",
    "\n",
    "Можно использовать динамическое партиционирование \n",
    "\n",
    "```sql\n",
    "SET hive.exec.dynamic.partition.mode=nonstrict;\n",
    "\n",
    "CREATE TABLE cik_ext(\n",
    "    uik string, \n",
    "    valid INT, \n",
    "    putin INT, \n",
    "    grudinin INT) \n",
    " partitioned BY (region string);\n",
    " \n",
    "INSERT overwrite TABLE cik_ext partition(region) \n",
    "SELECT uik , valid, putin, grudinin, region\n",
    "FROM cik;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skewed tables\n",
    "\n",
    "```sql\n",
    "CREATE TABLE skewed_table (col1 STRING, col2 INT, col3 STRING)\n",
    "  SKEWED BY (col1, col2) ON (('s1',1), ('s3',3), ('s13',13), ('s78',78)) STORED AS DIRECTORIES;\n",
    "```                                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketed table\n",
    "\n",
    "```sql\n",
    "CREATE TABLE page_view(\n",
    "    viewTime INT, \n",
    "    userid BIGINT,\n",
    "    page_url STRING, \n",
    "    referrer_url STRING,\n",
    "    p STRING) \n",
    " PARTITIONED BY(dt STRING, country STRING)\n",
    " CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS\n",
    " STORED AS SEQUENCEFILE;\n",
    " ```\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных\n",
    "\n",
    "```sql\n",
    "LOAD DATA LOCAL INPATH '/page_views.csv'\n",
    "OVERWRITE INTO TABLE page_view\n",
    "PARTITION (dt='2017-05-05', country='RU');\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert\n",
    "\n",
    "```sql\n",
    "\n",
    "CREATE TABLE pageviews (\n",
    "    userid VARCHAR(64), \n",
    "    link STRING, \n",
    "    came_from STRING\n",
    "  ) \n",
    "  PARTITIONED BY (datestamp STRING) \n",
    "  CLUSTERED BY (userid) INTO 256 BUCKETS \n",
    "  STORED AS ORC;\n",
    " \n",
    "INSERT INTO TABLE pageviews PARTITION (datestamp = '2014-09-23')\n",
    "  VALUES ('jsmith', 'mail.com', 'sports.com'), \n",
    "         ('jdoe', 'mail.com', null);\n",
    " \n",
    "INSERT INTO TABLE pageviews PARTITION (datestamp)\n",
    "  VALUES ('tjohnson', 'sports.com', 'finance.com', '2014-09-23'), \n",
    "         ('tlee', 'finance.com', null, '2014-09-21');\n",
    "         \n",
    "         \n",
    "         \n",
    "FROM old_pageviews\n",
    "  INSERT OVERWRITE TABLE pageviews_1 PARTITION (datestamp = '2014-09-23')\n",
    "      SELECT * WHERE came_from='google.com'\n",
    "  INSERT OVERWRITE TABLE pageviews_2 PARTITION (datestamp = '2014-09-23')\n",
    "      SELECT * WHERE came_from='yandex.ru';         \n",
    "  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Views\n",
    "\n",
    "\n",
    "```sql\n",
    "CREATE VIEW onion_referrers(url)\n",
    "  AS\n",
    "  SELECT DISTINCT referrer_url\n",
    "  FROM page_view\n",
    "  WHERE page_url='http://www.example.com';\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins\n",
    "\n",
    "```sql\n",
    "\n",
    "CREATE TABLE a (k1 string, v1 string);\n",
    "CREATE TABLE b (k2 string, v2 string);\n",
    "\n",
    "SELECT k1, v1, k2, v2\n",
    "FROM a JOIN b ON k1 = k2; \n",
    "\n",
    "SELECT a.* FROM a JOIN b ON (a.id = b.id AND a.department = b.department)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF\n",
    "\n",
    "Функции в `HiveSQL`, которые могут определять пользователи:\n",
    "\n",
    "`our_udf.py`\n",
    "```python \n",
    "\n",
    "import sys\n",
    "\n",
    "# оставляем только последнее слово\n",
    "for line in sys.stdin:\n",
    "    for val in line.strip().split('\\t'):\n",
    "        print(val.split(' ')[-1])\n",
    "```\n",
    "\n",
    "\n",
    "В консоли пишем:\n",
    "```sql\n",
    "ADD FILE our_udf.py;\n",
    "\n",
    "SELECT TRANSFORM(region, uik) USING 'python3 out_udf.py' AS (h STRING) FROM cik;\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
